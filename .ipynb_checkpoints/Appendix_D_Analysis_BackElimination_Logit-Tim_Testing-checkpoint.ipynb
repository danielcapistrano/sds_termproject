{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L_-ijuGTfnFV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn.datasets\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importing datasets\n",
    "After data cleaning and transformation (Appendix B) and sampling (Appendix C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s5CDY99xfnFZ"
   },
   "outputs": [],
   "source": [
    "# Creating list with names of files exported after sampling\n",
    "csv_files = [file for file in os.listdir(\"./data/csv_balanced/\") if file.endswith('.csv')]\n",
    "\n",
    "# Creating an empty list to store dataframes\n",
    "list_df = []\n",
    "\n",
    "# Loop to import each file and append to the created list\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(os.path.join(\"./data/csv_balanced/\", csv))\n",
    "    list_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Standardising features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_std = []\n",
    "\n",
    "for df in list_df:\n",
    "    std_scaler = StandardScaler()\n",
    "    cols = df.columns.difference(['fraud_bool'])\n",
    "    df[cols] = std_scaler.fit_transform(df[cols])\n",
    "    list_std.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Correlation analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to plot correlation heatmap of one of the dataframes in the list\n",
    "def plot_heatmap(dataframe_number):\n",
    "    f,ax = plt.subplots(figsize=(15, 15))\n",
    "    heatmap = sns.heatmap(list_std[dataframe_number].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
    "    heatmap.set_title('Correlation heatmap', fontdict={'fontsize':20}, pad=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot heatmap\n",
    "plot_heatmap(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying pairs with high correlation \n",
    "\n",
    "def get_highcorr(dataframe_number):\n",
    "    corr = list_std[dataframe_number].corr().abs()\n",
    "    tab = corr.unstack()\n",
    "    print(tab[tab < 1].sort_values(ascending = False).head(10))\n",
    "\n",
    "get_highcorr(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVF analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Add a constant term to the data\n",
    "df_with_const = add_constant(df)\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = df_with_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_with_const.values, i) for i in range(df_with_const.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Logistic regression with backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the inf columns!\n",
    "# cols_to_drop = ['INTERNET', 'TELEAPP', 'linux', 'other', 'windows', 'x11', 'macintosh', 'velocity_4w', 'month']\n",
    "# list_std = [df.drop(columns=cols_to_drop) for df in list_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function specifying the model\n",
    "def backward_elimination(X, y, sig_lvl=0.05):\n",
    "    num_vars = len(X.columns)\n",
    "    for i in range(0, num_vars):\n",
    "        regressor_Logit = sm.Logit(y.astype(float), X.astype(float)).fit()\n",
    "        max_var = max(regressor_Logit.pvalues)\n",
    "        print(max_var)\n",
    "        print(type(max_var))\n",
    "        if max_var > sig_lvl or pd.isnull(max_var):\n",
    "            for j in range(0, num_vars - i):\n",
    "                if regressor_Logit.pvalues[j].astype(float) == max_var or pd.isnull(regressor_Logit.pvalues[j]):\n",
    "                    print(f\"dropping {X.columns[j]}\")\n",
    "                    X = X.drop(X.columns[j], axis=1)\n",
    "                    break\n",
    "                    \n",
    "    return(regressor_Logit, X, y)\n",
    "\n",
    "# Creating function to run the model using a given dataframe\n",
    "def run_model(dataframe):\n",
    "    # Separate features (X) and target variable (Y)\n",
    "    X = dataframe.drop('fraud_bool', axis=1)\n",
    "    y = dataframe['fraud_bool']\n",
    "\n",
    "    # Add constant column to X for intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Perform backward elimination\n",
    "    return(backward_elimination(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model through the list of dataframes\n",
    "list_models = [run_model(df) for df in list_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing regression results\n",
    "[print(model.summary().tables[0]) for model, X, y in list_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing features' coefficients\n",
    "[print(model.summary().tables[1]) for model, X, y in list_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "cross_tabs = []\n",
    "for i, model_tuple in enumerate(list_models):\n",
    "    logit_model = model_tuple[0]\n",
    "    X = model_tuple[1]\n",
    "    y = model_tuple[2]\n",
    "    df = list_std[0]\n",
    "\n",
    "    df.loc[:, 'probability'] = logit_model.predict(X)\n",
    "    df.loc[:, 'yhat'] = (df.probability > 0.5) * 1\n",
    "    ct = pd.crosstab(df.fraud_bool, df.yhat)\n",
    "    cross_tabs.append(ct)\n",
    "    print(ct)\n",
    "\n",
    "    # Find accuracy of the model using formula ACC=(TP+TN)/N\n",
    "    acc = (ct.iloc[0][0] + ct.iloc[1][1]) / df.shape[0]\n",
    "    accuracies.append(acc)\n",
    "    print('Accuracy of the model is', acc, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average accuracy for Fraud is {sum([ct.iloc[0][0] for ct in cross_tabs])/sum([ct.iloc[0][0]+ct.iloc[0][1] for ct in cross_tabs])}\")\n",
    "print(f\"Average accuracy for non-Fraud is {sum([ct.iloc[1][1] for ct in cross_tabs])/sum([ct.iloc[1][0]+ct.iloc[1][1] for ct in cross_tabs])}\")\n",
    "print(f\"Average accuracy is {sum(accuracies)/len(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
